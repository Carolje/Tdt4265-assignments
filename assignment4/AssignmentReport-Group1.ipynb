{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 Report - Group 112"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "The intersection over the union (IoU) is a measure of how well two areas overlap (the closer to one, the more overlap). It is done by taking the ratio of the area of intersection and the area of the union. That is for two areas A and B the IoU will be, \n",
    "$\\text{IoU} = \\frac{A \\cap B}{A \\cup B}$.\n",
    "The bounding box $A$ can be represented by the two coordinate pairs $(x_{A1}, y_{A1})$ and $(x_{A2}, y_{A2})$, and B by $(x_{B1}, y_{B1})$ and $(x_{B2}, y_{B2})$. Meaning that the area of A is $(x_{A2} - x_{A2})\\cdot (y_{A1} - y_{A2})$ and the area of B is $(x_{B2} - x_{B2})\\cdot (y_{B1} - y_{B2})$\n",
    "then for the figure below, the IoU will be,\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{IoU} &= \\frac{A\\cap B}{A\\cup B} = \\frac{A\\cap B}{A + B - A\\cap B}\\\\\n",
    "&=\\frac{(x_{A2} - x_{B1})\\cdot (y_{B1} - y_{A2})}{\n",
    "(x_{A2} - x_{A2})\\cdot (y_{A1} - y_{A2}) + (x_{B2} - x_{B2})\\cdot (y_{B1} - y_{B2}) - (x_{A2} - x_{B1})\\cdot (y_{B1} - y_{A1})\n",
    "}\n",
    "\\end{align*}\n",
    "![](IoUdrawing.jpg)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1b)\n",
    "A true positive (TP) is when our classifiser correctly classify the data to the correct class (positve class), while a negative positive (TN) is when our classifiser correctly classify the data to not be in a class.\n",
    "The precision is the rate between all true positive findings for a given class and all positive findings (true and false) for that same class, that is the precentage of correct classifications for that class and it is given by,\n",
    "$\\text{Precision } = \\frac{TP}{TP + FP}$.\n",
    "Recall is the rate between all true positive findings for a class and all instances of that class, in other words recall is a measure of how well our classifiser find all positive cases of a class and it is then given by\n",
    "$\\text{Recall } = \\frac{TP}{TP + FN}$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1c)\n",
    "\n",
    "The calculation for the mAP is shown in the picture below\n",
    "![](task2/mAPcalc.jpg)\n",
    "Thus the for these PR-curves are $0.641$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "### Task 2f)\n",
    "Of this PR-curve we get an mAP of 0.907 which is slightly higher than it should be, but within reasonable limits.\n",
    "![](task2/precision_recall_curve.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3a)\n",
    "The filtering method is called for non-maximum suppression (nms) and is done with a jacard overlap threshold.\n",
    "\n",
    "### Task 3b)\n",
    "TRUE\n",
    "\n",
    "### Task 3c)\n",
    "Diffrent sizes of bounding boxes allows for detections of objects with different shapes, so a set of default bounding boxes applied at the same anchor lets us identify different objects more efficiently.\n",
    "\n",
    "\n",
    "### Task 3d)\n",
    "The diffrence between SSD and YOLOv1/v2 is that YOLO does not use a set of predefined default bounding boxes, but instead uses K-means clustering on the training-data to propose a set of default bounding boxes. This increases the training time of YOLOv1/v2. SSD also differ from YOLOv1/v2 in that it is able to detect objects of diffrent sizes more effciently since it has multi-scale feature maps, while YOLO only has single scale feature maps. SSD also uses a convolutional filter in order to predict the offset to the default bounding boxes, wheras YOLOv1/v2 uses a fully connected layer to do this. Lastly, the matching strategy is the different as YOLOv1 matches the prediction with ground truth based soley on the highest IOU while SSD uses nms with jacard overlap.\n",
    "\n",
    "### Task 3e)\n",
    "We have $38\\times 38\\times 6=8664$ bounding boxes as we have $38 \\times 38$ locactions and at each location we will have $6$ boxes.\n",
    "\n",
    "### Task 3f)\n",
    "Using a similar approach as in the previous subtask, at each feature map we will have $H \\times W \\times 6$ anchor boxes, thus in total we will get the sum of all anchor boxes in each feature map which is\n",
    "\n",
    "$$(38 \\times 38 +19 \\times 19 + 10 \\times 10 + 5 \\times 5 + 3 \\times 3 + 1 \\times 1)\\times 6=11640$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "The plot for total loss\n",
    "\n",
    "![](4b_total_loss.png)\n",
    "\n",
    "Our mAP for 6000 iterations are $0.7256$.\n",
    "\n",
    "## Task 4c)\n",
    "To improve the accuracy of our model we first did some data augmentation where we flipped some images horizontally, rotated some of the images and we used colorjitter. This improved the learning for the first 3000 iterations but had little effect on the mAP around 10000 iterations. Then we implemented a new $75 \\times 75$ output layer before the other layers. So our new structure is \n",
    "\n",
    "![](table1.png)\n",
    "![](table2.png)\n",
    "\n",
    "This new structure gave a mAP of $0.9361$ at 10K gradient descent iterations. \n",
    "\n",
    "## Task 4d)\n",
    "We have that our image is $300 \\times 300$ and the feature map has resolution $5 \\times 5$ and we have stride $64 \\times 64$. This gives us that the pixel values for the 25 center points for the anchor boxes are \n",
    "\n",
    "| Column\\row | 1          | 2          | 3           | 4           | 5           |\n",
    "|------------|------------|------------|-------------|-------------|-------------|\n",
    "| 1          | (32 x 32)  | (96 x 32)  | (160 x 32)  | (244 x 32)  | (278 x 32)  |\n",
    "| 2          | (32 x 96)  | (96 x 96)  | (160 x 96)  | (244 x 96)  | (278 x 96)  |\n",
    "| 3          | (32 x 160) | (96 x 160) | (160 x 160) | (244 x 160) | (278 x 160) |\n",
    "| 4          | (32 x 244) | (96 x 244) | (160 x 244) | (244 x 244) | (278 x 244) |\n",
    "| 5          | (22 x 278) | (86 x 278) | (150 x 278) | (214 x 278) | (278 x 278) |\n",
    "\n",
    "For each aspect ratio there are calculated $2+2=4$ anchor boxes. So for aspect ratio 2 we get anchor boxes with sizes\n",
    "- $[162,162]$\n",
    "- $[186,186]$\n",
    "- $[229,115]$\n",
    "- $[115,229]$\n",
    "\n",
    "For aspect ratio 3 we get anchor boxes with sizes\n",
    "- $[162,162]$\n",
    "- $[186,186]$\n",
    "- $[281,94]$\n",
    "- $[94,281]$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4e)\n",
    "Here are the classified images from the mnist data set.\n",
    "\n",
    "![](SSD/demo/mnist_output/0.png)\n",
    "![](SSD/demo/mnist_output/1.png)\n",
    "![](SSD/demo/mnist_output/2.png)\n",
    "![](SSD/demo/mnist_output/3.png)\n",
    "![](SSD/demo/mnist_output/4.png)\n",
    "![](SSD/demo/mnist_output/5.png)\n",
    "![](SSD/demo/mnist_output/6.png)\n",
    "![](SSD/demo/mnist_output/7.png)\n",
    "![](SSD/demo/mnist_output/8.png)\n",
    "![](SSD/demo/mnist_output/9.png)\n",
    "![](SSD/demo/mnist_output/10.png)\n",
    "![](SSD/demo/mnist_output/11.png)\n",
    "![](SSD/demo/mnist_output/12.png)\n",
    "![](SSD/demo/mnist_output/13.png)\n",
    "![](SSD/demo/mnist_output/14.png)\n",
    "\n",
    "As seen in the images there where several digits our model could not detect. We had 24 digits that were not detected where most of them were small, which were expected since the SSD network typically has problems detecting small objects. In general there did not seem to be any specific digit that were never detected or misclassified every time. \n",
    "We had a few misclassifications. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4f)\n",
    "FILL IN ANSWER. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tdt4265",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
